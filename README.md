# story-comprehension-neural-network

#### An approach to solve the Sentence-Cloze-Task

A four-sentence short story and two alternatives for the last, fifth sentence that ends the story are given.
The system needs to decide which of the two is correct. The training data consists of five-sentence short stories that
include the correct ending but there is no incorrect ending given.

#### Data
- The original paper about the Story Cloze task is available at [0]
- The training set is available at [1]
- The validation set of the original task is available at [2]
- The testing set generated by ETH 2018 is available at [6]

Our approach uses Skip Thought Model code [3] to embed the sentences into vectors.
The generated vectors for:
 - the training dataset [1] can be found at [4].
 - the validation dataset [2] can be found at [5].
 - the ETH test dataset [6] can be found at [7].
 - the story-cloze test set [8] can be found at [9].
They should be downloaded and decompressed in the ./data directory.

We have implemented two models:
- Model 1: Correct vs Wrong Ending Binary Classifier that can be found in ./model_correct_vs_wrong_ending_classifier.py
- Model 2: Alternative-Ending Aware Binary Classifier that can be found in ./model_alternative_ending_aware_classifier.py

In order to reconstruct the results when evaluating the story-cloze test set one should first train the models
on both the training and validation datasets. In order to do that for
- Model 1, one should set the flag FLAGS.use_training_dataset at line 28 in ./model_correct_vs_wrong_ending_classifier.py accordingly.
- Model 2, one should set the flag FLAGS.use_val_for_training at line 16 in ./model_alternative_ending_aware_classifier.py accordingly.

To test the Model 1 one should keep in mind that the story-cloze test set has labels and
and modify in ./test_correct_vs_wrong_ending_classifier.py at line 18 FLAGS.has_labels = True.
The checkpoint file should be change for both ./test_correct_vs_wrong_ending_classifier.py and ./test_alternative_ending_aware_classifier.py.
The flag "testing_embeddings_dir" should be set to the directories with the embeddings: eg. embeddings_test or embeddings_test_eth

In order to reconstruct the output for the ETH-NLU-2018 test set one should use the model generated by the Model 1 on
the validation data and set in ./test_correct_vs_wrong_ending_classifier.py at line 18 FLAGS.has_labels = False.

__________________________________________________________________________________________________________
If one wants to generate Skip Thought Embeddings for a new dataset, one should pursue the following steps:

Run the following commands in terminal:
./story-comprehension-neural-network/script_install_skip_thought_dependencies.sh
pip2 install -r ./requirements_skip_thought_embeddings.txt

- modify ./skip-thoughts/skipthoughts.py at line 23 and 24 with the path to the models downloaded
eg. absolute path of the ./skip-thoughts/models/
- modify ./generate_skip_thought_embeddings.py at line 106 with the appropriate paths
to the new dataset and the directories that will store the generated embeddings

Run the following commands in terminal:
python2 -c "import nltk; nltk.download('punkt')"
python2 generate_skip_thought_embeddings.py


[0]  https://arxiv.org/abs/1604.01696 <br/>
[1]  https://polybox.ethz.ch/index.php/s/l2wM4RIyI3pD7Tl/download <br/>
[2]  https://polybox.ethz.ch/index.php/s/02IVLdBAgVcsJAx/download <br/>
[3]  https://github.com/ryankiros/skip-thoughts <br/>
[4]  https://polybox.ethz.ch/index.php/s/GVlcA363bvMwaYR <br/>
[5]  https://polybox.ethz.ch/index.php/s/helYFHS8criZp7z <br/>
[6]  https://polybox.ethz.ch/index.php/s/AKbA8g7SeHwjU0R <br/>
[7]  https://polybox.ethz.ch/index.php/s/XfeWseNSZcndP4a <br/>
[8]  https://polybox.ethz.ch/index.php/s/h2gp3FpS3N7Xgiq <br/>
[9]  https://polybox.ethz.ch/index.php/s/PSbhfzycSUXGjuV <br/>